{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867f8815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.search.documents.indexes.models import (\n",
    "        SearchIndex,\n",
    "        SearchField,\n",
    "        SearchFieldDataType,\n",
    "        SimpleField,\n",
    "        SearchableField,\n",
    "        VectorSearch,\n",
    "        VectorSearchProfile,\n",
    "        HnswAlgorithmConfiguration,\n",
    "    )\n",
    "import pandas as pd\n",
    "from azure.core.exceptions import ResourceNotFoundError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef50119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_search_indexes():\n",
    "    # Create a SearchIndexClient\n",
    "    credential = AzureKeyCredential(key)\n",
    "    index_client = SearchIndexClient(service_endpoint, credential)\n",
    "    \n",
    "    # Get all indexes\n",
    "    indexes = list(index_client.list_indexes())\n",
    "\n",
    "    final_indexes = [index.name for index in indexes]\n",
    "        \n",
    "    return final_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f039ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_defect_documents():\n",
    "    df = pd.read_excel(\"defect.xlsx\")\n",
    "\n",
    "\n",
    "    data_as_dict = df.to_dict(orient='records')\n",
    "    df.columns = df.columns.str.replace(' ', '_')\n",
    "\n",
    "    # Convert the DataFrame to a list of dictionaries\n",
    "    data_as_dict = df.to_dict(orient='records')\n",
    "\n",
    "    for item in data_as_dict:\n",
    "        item[\"TitleVector\"] = get_embeddings(item[\"Title\"])\n",
    "\n",
    "    return data_as_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013463d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Retrieving all indexes...\")\n",
    "# get all indexes\n",
    "all_indexes = get_all_search_indexes()\n",
    "print(all_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b67549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bug_index(name: str):\n",
    "    fields = [\n",
    "        SimpleField(name=\"ID\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(name=\"Work_Item_Type\", type=SearchFieldDataType.String, filterable=True, facetable=True),\n",
    "        SearchableField(name=\"State\", type=SearchFieldDataType.String, filterable=True, facetable=True),\n",
    "        SearchableField(name=\"Area\", type=SearchFieldDataType.String, filterable=True),\n",
    "        SearchField(\n",
    "            name=\"TitleVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            vector_search_dimensions=1536,\n",
    "             vector_search_profile_name=\"my-vector-config\",\n",
    "    ),\n",
    "\n",
    "        SearchableField(name=\"Title\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"Description\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"Repro_Steps\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"Created_Date\", type=SearchFieldDataType.String, sortable=True),\n",
    "        SearchableField(name=\"Created_By\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"Assigned_To\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"Iteration_Path\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"Severity\", type=SearchFieldDataType.String, filterable=True, facetable=True),\n",
    "        SearchableField(name=\"Priority\", type=SearchFieldDataType.Double),\n",
    "        SearchableField(name=\"Efforts\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"Comment\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"Closed_By\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"Closed_Date\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"Closing_Comment\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"Reason\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"Story_Points\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"Parent_Feature_Id\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"Parent_Feature_Title\", type=SearchFieldDataType.String),\n",
    " ]\n",
    "\n",
    "    vector_search = VectorSearch(\n",
    "        profiles=[VectorSearchProfile(name=\"my-vector-config\", algorithm_configuration_name=\"my-algorithms-config\")],\n",
    "        algorithms=[HnswAlgorithmConfiguration(name=\"my-algorithms-config\")],\n",
    " )\n",
    "\n",
    "    return SearchIndex(name=name, fields=fields, vector_search=vector_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Configure OpenAI\n",
    "\n",
    "\n",
    "# Replace with your actual search service admin key\n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "\n",
    "def get_embeddings(text: str):\n",
    "    \"\"\"Get embeddings for text using Azure OpenAI\"\"\"\n",
    "    if not text or text.strip() == \"\":\n",
    "        # Return a zero vector if text is empty\n",
    "        return [0.0] * 1536  # text-embedding-ada-002 returns 1536 dimensions\n",
    "    \n",
    "    try:\n",
    "        response = openai.Embedding.create(\n",
    "            input=text.strip(), \n",
    "            engine=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embeddings for text '{text[:50]}...': {str(e)}\")\n",
    "        # Return zero vector as fallback\n",
    "        return [0.0] * 1536\n",
    "\n",
    "def safe_value(val):\n",
    "    \"\"\"Safely convert values to strings, handling NaN and None\"\"\"\n",
    "    if pd.isna(val) or val in [np.nan, None]:\n",
    "        return \"\"\n",
    "    if isinstance(val, (pd.Timestamp, datetime)):\n",
    "        return val.isoformat()\n",
    "    # Ensure we return a string and handle any potential JSON serialization issues\n",
    "    try:\n",
    "        result = str(val)\n",
    "        # Remove any problematic characters that might cause JSON issues\n",
    "        return result.replace('\\x00', '').strip()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def safe_numeric_value(val, default=0):\n",
    "    \"\"\"Safely convert values to numeric, handling NaN and None\"\"\"\n",
    "    if pd.isna(val) or val in [np.nan, None]:\n",
    "        return default\n",
    "    try:\n",
    "        return float(val) if val != \"\" else default\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "def get_defect_documents():\n",
    "    \"\"\"Process Excel file and return documents for Azure Search\"\"\"\n",
    "    df = pd.read_excel(\"defect-new.xlsx\")\n",
    "    df.columns = df.columns.str.replace(' ', '_')\n",
    "    data = df.to_dict(orient='records')\n",
    "\n",
    "    transformed_data = []\n",
    "    for i, item in enumerate(data):\n",
    "        title = item.get('Title', 'No Title')\n",
    "        print(f\"Processing item {i+1}/{len(data)}: {title}\")\n",
    "        \n",
    "        # Get embeddings for the title\n",
    "        title_vector = get_embeddings(title)\n",
    "        \n",
    "        transformed_item = {\n",
    "            \"ID\": safe_value(item.get(\"ID\")),\n",
    "            \"Work_Item_Type\": safe_value(item.get(\"Work_Item_Type\")),\n",
    "            \"State\": safe_value(item.get(\"State\")),\n",
    "            \"Area\": safe_value(item.get(\"Area\")),\n",
    "            \"Title\": safe_value(item.get(\"Title\")),\n",
    "            \"Description\": safe_value(item.get(\"Description\")) or \"No description provided\",\n",
    "            \"Repro_Steps\": safe_value(item.get(\"Repro_Steps\")),\n",
    "            \"Created_Date\": safe_value(item.get(\"Created_Date\")),\n",
    "            \"Created_By\": safe_value(item.get(\"Created_By\")),\n",
    "            \"Assigned_To\": safe_value(item.get(\"Assigned_To\")),\n",
    "            \"Iteration_Path\": safe_value(item.get(\"Iteration_Path\")),\n",
    "            \"Severity\": safe_value(item.get(\"Severity\")),\n",
    "            \"Priority\": safe_value(item.get(\"Priority\")),\n",
    "            \"Efforts\": safe_numeric_value(item.get(\"Efforts\"), 0),\n",
    "            \"Comment\": safe_value(item.get(\"Comment\")),\n",
    "            \"Closed_By\": safe_value(item.get(\"Closed_By\")),\n",
    "            \"Closed_Date\": safe_value(item.get(\"Closed_Date\")),\n",
    "            \"Closing_Comment\": safe_value(item.get(\"Closing_Comment\")),\n",
    "            \"Reason\": safe_value(item.get(\"Reason\")),\n",
    "            \"Story_Points\": safe_numeric_value(item.get(\"Story_Points\"), 0),\n",
    "            \"Parent_Feature_Id\": safe_value(item.get(\"Parent_Feature_Id\")),\n",
    "            \"Parent_Feature_Title\": safe_value(item.get(\"Parent_Feature_Title\")),\n",
    "            \"TitleVector\": title_vector\n",
    "        }\n",
    "        transformed_data.append(transformed_item)\n",
    "\n",
    "    return transformed_data\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to upload documents to Azure Search\"\"\"\n",
    "    try:\n",
    "        # Create SearchClient\n",
    "        print(f\"Service endpoint: {service_endpoint}\")\n",
    "        print(f\"Index name: {index_name}\")\n",
    "        client = SearchClient(service_endpoint, index_name, credential)\n",
    "        \n",
    "        # Get defect documents\n",
    "        defect_docs = get_defect_documents()\n",
    "        \n",
    "        print(\"Uploading defect documents to the index...\")\n",
    "        \n",
    "        # Upload documents in smaller batches to avoid size limits\n",
    "        batch_size = 10  # Adjust based on your document size\n",
    "        for i in range(0, len(defect_docs), batch_size):\n",
    "            batch = defect_docs[i:i + batch_size]\n",
    "            print(f\"Uploading batch {i//batch_size + 1}/{(len(defect_docs) + batch_size - 1)//batch_size}\")\n",
    "            result = client.upload_documents(documents=batch)\n",
    "            \n",
    "            # Check for any failures\n",
    "            for doc_result in result:\n",
    "                if not doc_result.succeeded:\n",
    "                    print(f\"Failed to upload document {doc_result.key}: {doc_result.error_message}\")\n",
    "        \n",
    "        print(\"Defect documents uploaded successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during upload: {str(e)}\")\n",
    "        print(\"Please check your credentials and index configuration.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f380fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "\n",
    "credential = AzureKeyCredential(admin_key)\n",
    "\n",
    "def get_embeddings(text: str):\n",
    "    \"\"\"Get embeddings for text using Azure OpenAI\"\"\"\n",
    "    if not text or text.strip() == \"\":\n",
    "        # Return a zero vector if text is empty\n",
    "        return [0.0] * 1536  # text-embedding-ada-002 returns 1536 dimensions\n",
    "    \n",
    "    try:\n",
    "        response = openai.Embedding.create(\n",
    "            input=text.strip(), \n",
    "            engine=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embeddings for text '{text[:50]}...': {str(e)}\")\n",
    "        # Return zero vector as fallback\n",
    "        return [0.0] * 1536\n",
    "\n",
    "def safe_value(val):\n",
    "    \"\"\"Safely convert values to strings, handling NaN and None\"\"\"\n",
    "    if pd.isna(val) or val in [np.nan, None]:\n",
    "        return \"\"\n",
    "    if isinstance(val, (pd.Timestamp, datetime)):\n",
    "        return val.isoformat()\n",
    "    # Ensure we return a string and handle any potential JSON serialization issues\n",
    "    try:\n",
    "        result = str(val)\n",
    "        # Remove any problematic characters that might cause JSON issues\n",
    "        return result.replace('\\x00', '').strip()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def safe_numeric_value(val, default=\"0\"):\n",
    "    \"\"\"Safely convert values to string representation of numeric, handling NaN and None\"\"\"\n",
    "    if pd.isna(val) or val in [np.nan, None]:\n",
    "        return default\n",
    "    try:\n",
    "        # Convert to float first to handle numeric values, then to string\n",
    "        if val == \"\" or val == \"\":\n",
    "            return default\n",
    "        numeric_val = float(val)\n",
    "        # Convert to int if it's a whole number, otherwise keep as float\n",
    "        if numeric_val.is_integer():\n",
    "            return str(int(numeric_val))\n",
    "        else:\n",
    "            return str(numeric_val)\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "def get_defect_documents():\n",
    "    \"\"\"Process Excel file and return documents for Azure Search\"\"\"\n",
    "    df = pd.read_excel(\"defect.xlsx\")\n",
    "    df.columns = df.columns.str.replace(' ', '_')\n",
    "    data = df.to_dict(orient='records')\n",
    "\n",
    "    transformed_data = []\n",
    "    for i, item in enumerate(data):\n",
    "        title = item.get('Title', 'No Title')\n",
    "        print(f\"Processing item {i+1}/{len(data)}: {title}\")\n",
    "        \n",
    "        # Get embeddings for the title\n",
    "        title_vector = get_embeddings(title)\n",
    "        \n",
    "        transformed_item = {\n",
    "            \"ID\": safe_value(item.get(\"ID\")),\n",
    "            \"Work_Item_Type\": safe_value(item.get(\"Work_Item_Type\")),\n",
    "            \"State\": safe_value(item.get(\"State\")),\n",
    "            \"Area\": safe_value(item.get(\"Area\")),\n",
    "            \"Title\": safe_value(item.get(\"Title\")),\n",
    "            \"Description\": safe_value(item.get(\"Description\")) or \"No description provided\",\n",
    "            \"Repro_Steps\": safe_value(item.get(\"Repro_Steps\")),\n",
    "            \"Created_Date\": safe_value(item.get(\"Created_Date\")),\n",
    "            \"Created_By\": safe_value(item.get(\"Created_By\")),\n",
    "            \"Assigned_To\": safe_value(item.get(\"Assigned_To\")),\n",
    "            \"Iteration_Path\": safe_value(item.get(\"Iteration_Path\")),\n",
    "            \"Severity\": safe_value(item.get(\"Severity\")),\n",
    "            \"Priority\": safe_value(item.get(\"Priority\")),\n",
    "            \"Efforts\": safe_numeric_value(item.get(\"Efforts\"), \"0\"),\n",
    "            \"Comment\": safe_value(item.get(\"Comment\")),\n",
    "            \"Closed_By\": safe_value(item.get(\"Closed_By\")),\n",
    "            \"Closed_Date\": safe_value(item.get(\"Closed_Date\")),\n",
    "            \"Closing_Comment\": safe_value(item.get(\"Closing_Comment\")),\n",
    "            \"Reason\": safe_value(item.get(\"Reason\")),\n",
    "            \"Story_Points\": safe_numeric_value(item.get(\"Story_Points\"), \"0\"),\n",
    "            \"Parent_Feature_Id\": safe_value(item.get(\"Parent_Feature_Id\")),\n",
    "            \"Parent_Feature_Title\": safe_value(item.get(\"Parent_Feature_Title\")),\n",
    "            \"TitleVector\": title_vector\n",
    "        }\n",
    "        transformed_data.append(transformed_item)\n",
    "\n",
    "    return transformed_data\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to upload documents to Azure Search\"\"\"\n",
    "    try:\n",
    "        # Create SearchClient\n",
    "        print(f\"Service endpoint: {service_endpoint}\")\n",
    "        print(f\"Index name: {index_name}\")\n",
    "        client = SearchClient(service_endpoint, index_name, credential)\n",
    "        \n",
    "        # Get defect documents\n",
    "        defect_docs = get_defect_documents()\n",
    "        \n",
    "        print(\"Uploading defect documents to the index...\")\n",
    "        \n",
    "        # Upload documents in smaller batches to avoid size limits\n",
    "        batch_size = 10  # Adjust based on your document size\n",
    "        for i in range(0, len(defect_docs), batch_size):\n",
    "            batch = defect_docs[i:i + batch_size]\n",
    "            print(f\"Uploading batch {i//batch_size + 1}/{(len(defect_docs) + batch_size - 1)//batch_size}\")\n",
    "            result = client.upload_documents(documents=batch)\n",
    "            \n",
    "            # Check for any failures\n",
    "            for doc_result in result:\n",
    "                if not doc_result.succeeded:\n",
    "                    print(f\"Failed to upload document {doc_result.key}: {doc_result.error_message}\")\n",
    "        \n",
    "        print(\"Defect documents uploaded successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during upload: {str(e)}\")\n",
    "        print(\"Please check your credentials and index configuration.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = AzureKeyCredential(key)\n",
    "index_client = SearchIndexClient(service_endpoint, credential)\n",
    "index = get_bug_index(index_name)\n",
    "print(type(index))\n",
    "\n",
    "print(\"Creating the index if it does not exist\")\n",
    "try:\n",
    "    index_client.get_index(index.name)\n",
    "    print(f\"Index '{index.name}' already exists.\")\n",
    "except ResourceNotFoundError:\n",
    "    index_client.create_index(index)\n",
    "    print(f\"Index '{index.name}' created successfully.\")\n",
    "\n",
    "\n",
    "print(f\"Index {index.name} created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c56ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_client.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44782845",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = get_bug_index(index_name)\n",
    "try:\n",
    "    index_client.get_index(index.name)\n",
    "except ResourceNotFoundError:\n",
    "    print(\"creating a new one\")\n",
    "    index_client.create_index(index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6190d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_index_fields(index_name):\n",
    "    \"\"\"\n",
    "    Get the existing index schema to understand current field structure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        credential = AzureKeyCredential(key)\n",
    "        index_client = SearchIndexClient(service_endpoint, credential)\n",
    "        existing_index = index_client.get_index(index_name)\n",
    "        \n",
    "        print(f\"Existing index '{index_name}' fields:\")\n",
    "        for field in index.fields:\n",
    "            print(f\"Name: {field.name}, Type: {field.type}, Key: {field.key}, Searchable: {field.searchable}, Filterable: {field.filterable}\")\n",
    "\n",
    "        \n",
    "        return existing_index\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting existing index: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_existing_index_fields(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac845e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad87a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def safe_value(val):\n",
    "    if pd.isna(val) or val in [np.nan, None]:\n",
    "        return \"\"\n",
    "    if isinstance(val, (pd.Timestamp, datetime)):\n",
    "        return val.isoformat()\n",
    "    return str(val)\n",
    "\n",
    "def get_defect_documents():\n",
    "    df = pd.read_excel(\"defect-new.xlsx\")\n",
    "    df.columns = df.columns.str.replace(' ', '_')\n",
    "    data = df.to_dict(orient='records')\n",
    "\n",
    "    transformed_data = []\n",
    "    for i, item in enumerate(data):\n",
    "        print(f\"Processing item {i+1}/{len(data)}: {item.get('Title', 'No Title')}\")\n",
    "        transformed_item = {\n",
    "            \"ID\": safe_value(item.get(\"ID\")),\n",
    "            \"Work_Item_Type\": safe_value(item.get(\"Work_Item_Type\")),\n",
    "            \"State\": safe_value(item.get(\"State\")),\n",
    "            \"Area\": safe_value(item.get(\"Area\")),\n",
    "            \"Title\": safe_value(item.get(\"Title\")),\n",
    "            \"Description\": item.get(\"Description\") or \"No description provided\",\n",
    "            \"Repro_Steps\": safe_value(item.get(\"Repro_Steps\")),\n",
    "            \"Created_Date\": safe_value(item.get(\"Created_Date\")),\n",
    "            \"Created_By\": safe_value(item.get(\"Created_By\")),\n",
    "            \"Assigned_To\": safe_value(item.get(\"Assigned_To\")),\n",
    "            \"Iteration_Path\": safe_value(item.get(\"Iteration_Path\")),\n",
    "            \"Severity\": safe_value(item.get(\"Severity\")),\n",
    "            \"Priority\": safe_value(item.get(\"Priority\")),\n",
    "            \"Efforts\": item.get(\"Efforts\") if pd.notna(item.get(\"Efforts\")) else 0,\n",
    "            \"Comment\": safe_value(item.get(\"Comment\")),\n",
    "            \"Closed_By\": safe_value(item.get(\"Closed_By\")),\n",
    "            \"Closed_Date\": safe_value(item.get(\"Closed_Date\")),\n",
    "            \"Closing_Comment\": safe_value(item.get(\"Closing_Comment\")),\n",
    "            \"Reason\": safe_value(item.get(\"Reason\")),\n",
    "            \"Story_Points\": item.get(\"Story_Points\") if pd.notna(item.get(\"Story_Points\")) else 0,\n",
    "            \"Parent_Feature_Id\": safe_value(item.get(\"Parent_Feature_Id\")),\n",
    "            \"Parent_Feature_Title\": safe_value(item.get(\"Parent_Feature_Title\")),\n",
    "            \"TitleVector\": get_embeddings(item.get(\"Title\", \"\"))\n",
    "        }\n",
    "        transformed_data.append(transformed_item)\n",
    "\n",
    "    return transformed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c0c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SearchClient to interact with the index\n",
    "print(service_endpoint)\n",
    "print(index_name)\n",
    "client = SearchClient(service_endpoint, index_name, credential)\n",
    "defect_docs = get_defect_documents()\n",
    "print(\"Uploading defect documents to the index...\")\n",
    "# create embeddings\n",
    "client.upload_documents(documents=defect_docs)\n",
    "\n",
    "print(\"defects documents uploaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f71c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_defect_documents():\n",
    "    \"\"\"\n",
    "    Prepare defect data for indexing with proper field name mapping\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        {\n",
    "            \"Work_Item_Type\": \"Bug\",\n",
    "            \"ID\": 897,\n",
    "            \"State\": \"Closed\",\n",
    "            \"Area\": \"Gen AI QEP 2.0\\\\Create - Existing Solution Enhancements\",\n",
    "            \"Title\": \"When a solution is created on a name which is already existing, the sysyem should give an api error alert. \",\n",
    "            \"Description\": None,\n",
    "            \"Repro Steps\": \"<div><span style=\\\"display:inline !important;\\\">When a user creates a solution by a duplicate name, there should be an alert message &quot;Solution already exists&quot;.</span><br> </div>\",\n",
    "            \"Created Date\": \"11/27/2023 5:43:52 PM\",\n",
    "            \"Created By\": None,\n",
    "            \"Assigned To\": None,\n",
    "            \"Iteration Path\": \"Gen AI QEP 2.0\\\\PI-2.1\",\n",
    "            \"Severity\": \"3 - Medium\",\n",
    "            \"Priority\": 2.0,\n",
    "            \"Efforts\": None,\n",
    "            \"Comment\": None,\n",
    "            \"Closed By\": \"Siva Reddy Dirisanapu <siva-reddy.dirisanapu@capgemini.com>\",\n",
    "            \"Closed Date\": \"2023-01-12T16:25:11\",\n",
    "            \"Closing Comment\": None,\n",
    "            \"Reason\": \"Fixed and verified\",\n",
    "            \"Story Points\": None,\n",
    "            \"Parent Feature Id\": 555.0,\n",
    "            \"Parent Feature Title\": None\n",
    "        }\n",
    "    ]\n",
    "    print(type(data))\n",
    "\n",
    "    # Transform data to match existing index structure\n",
    "    transformed_data = []\n",
    "    for i, item in enumerate(data):\n",
    "        transformed_item = {\n",
    "            \"ID\": str(item[\"ID\"]),\n",
    "            \"Work_Item_Type\": item[\"Work_Item_Type\"],\n",
    "            \"State\": item[\"State\"],\n",
    "            \"Area\": item[\"Area\"],\n",
    "            \"Title\": item[\"Title\"],\n",
    "            \"Description\": item[\"Description\"] or \"No description provided\",\n",
    "            \"Repro_Steps\": item[\"Repro Steps\"],\n",
    "            \"Created_Date\": item[\"Created Date\"],\n",
    "            \"Created_By\": item[\"Created By\"],\n",
    "            \"Assigned_To\": item[\"Assigned To\"],\n",
    "            \"Iteration_Path\": item[\"Iteration Path\"],\n",
    "            \"Severity\": item[\"Severity\"],\n",
    "            \"Priority\": str(item[\"Priority\"]),\n",
    "            \"Efforts\": item[\"Efforts\"],\n",
    "            \"Comment\": item[\"Comment\"],\n",
    "            \"Closed_By\": item[\"Closed By\"],\n",
    "            \"Closed_Date\": item[\"Closed Date\"],\n",
    "            \"Closing_Comment\": item[\"Closing Comment\"],\n",
    "            \"Reason\": item[\"Reason\"],\n",
    "            \"Story_Points\": item[\"Story Points\"],\n",
    "            \"Parent_Feature_Id\": str(item[\"Parent Feature Id\"]),\n",
    "            \"Parent_Feature_Title\": item[\"Parent Feature Title\"],\n",
    "            \"TitleVector\": get_embeddings(item[\"Title\"])  # Use existing vector field\n",
    "        }\n",
    "        transformed_data.append(transformed_item)\n",
    "\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf26d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SearchClient to interact with the index\n",
    "print(service_endpoint)\n",
    "print(index_name)\n",
    "client = SearchClient(service_endpoint, index_name, credential)\n",
    "defect_docs = get_defect_documents()\n",
    "print(defect_docs)\n",
    "print(\"Uploading defect documents to the index...\")\n",
    "# create embeddings\n",
    "client.upload_documents(documents=defect_docs)\n",
    "\n",
    "print(\"defects documents uploaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58626f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_defect_documents():\n",
    "    \"\"\"\n",
    "    Retrieves all defect documents from the Azure Cognitive Search index.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"key is ........{key}\")\n",
    "    print(f\"service_endpoint is ........{service_endpoint}\")\n",
    "    try:\n",
    "        credential = AzureKeyCredential(key)\n",
    "        search_client = SearchClient(service_endpoint, index_name, credential)\n",
    "\n",
    "        # search_text=\"*\" matches all documents\n",
    "        results = search_client.search(\n",
    "            search_text=\"*\",\n",
    "            select=[\"ID\", \"Title\", \"Description\", \"Severity\", \"State\", \"Work_Item_Type\"],\n",
    "            top=1000\n",
    "        )\n",
    "\n",
    "        documents = list(results)\n",
    "\n",
    "        print(f\"Retrieved {len(documents)} defect document(s).\")\n",
    "        for doc in documents:\n",
    "            print(f\"  - ID: {doc['ID']}, Title: {doc.get('Title', 'N/A')}, Severity: {doc.get('Severity', 'N/A')}, State: {doc.get('State', 'N/A')}\")\n",
    "\n",
    "        return documents\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(f\"Error retrieving defect documents: {ex}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc92e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_defect_documents_by_id(document_ids):\n",
    "    \"\"\"\n",
    "    Retrieves defect documents by ID\n",
    "    \"\"\"\n",
    "    try:\n",
    "        credential = AzureKeyCredential(key)\n",
    "        search_client = SearchClient(service_endpoint, index_name, credential)\n",
    "\n",
    "        documents = []\n",
    "        for doc_id in document_ids:\n",
    "            try:\n",
    "                doc = search_client.get_document(key=str(doc_id))\n",
    "                documents.append(doc)\n",
    "                print(f\"  - ID: {doc['ID']}, Title: {doc.get('Title', 'N/A')}, Severity: {doc.get('Severity', 'N/A')}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  - Could not find defect document with ID {doc_id}: {e}\")\n",
    "\n",
    "        return documents\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(f\"Error: {ex}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc87d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_defect_documents_by_id([\"897\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_vector_search_defects(query):\n",
    "    \"\"\"\n",
    "    Perform vector search on defect titles\n",
    "    \"\"\"\n",
    "    search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=get_embeddings(query), \n",
    "        k_nearest_neighbors=5, \n",
    "        fields=\"TitleVector\"  # Use existing vector field\n",
    "    )\n",
    "\n",
    "    results = search_client.search(\n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"ID\", \"Title\", \"Description\", \"Severity\", \"State\"],\n",
    "    )\n",
    "\n",
    "    print(f\"Vector search results for query: '{query}'\")\n",
    "    for result in results:\n",
    "        print(f\"  - ID: {result.get('ID', 'N/A')}, Title: {result.get('Title', 'N/A')}, Score: {result.get('@search.score', 'N/A')}\")\n",
    "\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_vector_search_defects(query = \"API error alert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_vector_search_with_filter_defects(query=\"API error\", severity_filter=\"3 - Medium\"):\n",
    "    \"\"\"\n",
    "    Perform vector search with severity filter\n",
    "    \"\"\"\n",
    "    search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=get_embeddings(query),\n",
    "        k_nearest_neighbors=5,\n",
    "        fields=\"TitleVector\"  # Use existing vector field\n",
    "    )\n",
    "\n",
    "    results = search_client.search(\n",
    "        search_text=query,\n",
    "        vector_queries=[vector_query],\n",
    "        filter=f\"Severity eq '{severity_filter}'\",\n",
    "        select=[ \"ID\", \"Title\", \"Severity\", \"State\"],\n",
    "        top=5\n",
    "    )\n",
    "\n",
    "    print(f\"Filtered vector search results for query: '{query}' with severity: '{severity_filter}'\")\n",
    "    for result in results:\n",
    "        print(f\"  - ID: {result.get('ID', 'N/A')}, Title: {result.get('Title', 'N/A')}, Severity: {result.get('Severity', 'N/A')}\")\n",
    "\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_vector_search_with_filter_defects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619c3b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_query_hybrid_search_defects(queries):\n",
    "    \"\"\"\n",
    "    Perform hybrid search for multiple defect-related queries\n",
    "    \"\"\"\n",
    "    search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for query in queries:\n",
    "        vector_query = VectorizedQuery(\n",
    "            vector=get_embeddings(query), \n",
    "            k_nearest_neighbors=3, \n",
    "            fields=\"TitleVector\"  # Use existing vector field\n",
    "        )\n",
    "        \n",
    "        results = search_client.search(\n",
    "            search_text=query,\n",
    "            vector_queries=[vector_query],\n",
    "            select=[\"ID\", \"Title\", \"Severity\", \"State\"],\n",
    "            top=3\n",
    "        )\n",
    "        \n",
    "        query_results = list(results)\n",
    "        all_results.append({\n",
    "            \"query\": query,\n",
    "            \"results\": query_results\n",
    "        })\n",
    "        \n",
    "        print(f\"Results for defect query: '{query}'\")\n",
    "        for result in query_results:\n",
    "            print(f\"  - ID: {result.get('ID', 'N/A')}, Title: {result.get('Title', 'N/A')}, Severity: {result.get('Severity', 'N/A')}\")\n",
    "        print(\"---\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b11a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_query_hybrid_search_defects([\n",
    "    \"What happens if a solution is created with a duplicate name?\",\n",
    "    \"Is there an alert when trying to create an existing solution?\",\n",
    "    \"Defect related to duplicate solution creation in Gen AI QEP 2.0 PI-2.1\",\n",
    "    \"System behavior when solution name already exists\",\n",
    "    \"Missing API error for duplicate solution creation\",\n",
    "    \"Closed bugs with medium severity and fixed status in Gen AI QEP\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_defect_document(defect_id):\n",
    "    \"\"\"\n",
    "    Delete a defect document by hotelId (the key field)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        credential = AzureKeyCredential(key)\n",
    "        search_client = SearchClient(service_endpoint, index_name, credential)\n",
    "        \n",
    "        search_client.delete_documents(documents=[{\"hotelId\": str(defect_id)}])\n",
    "        print(f\"Defect document with ID '{defect_id}' deleted.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting defect document with ID '{defect_id}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642177a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_index(index_name):\n",
    "    \"\"\"\n",
    "    Delete the entire search index\n",
    "    \"\"\"\n",
    "    credential = AzureKeyCredential(key)\n",
    "    index_client = SearchIndexClient(service_endpoint, credential)\n",
    "\n",
    "    try:\n",
    "        index_client.delete_index(index_name)\n",
    "        print(f\"Index '{index_name}' deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting index '{index_name}': {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
